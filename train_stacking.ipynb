{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import preprocessingV1\n",
    "from utils.modelization import saveModel,loadModel,submitModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from scipy.stats import uniform, randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"datasets/train_radiomics_hipocamp.csv\")\n",
    "df_test = pd.read_csv(\"datasets/test_radiomics_hipocamp.csv\")\n",
    "\n",
    "X_train, y_train = preprocessingV1(df_train)\n",
    "\n",
    "X_test = preprocessingV1(df_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=4,random_state=2022)\n",
    "bagg_dt_model = BaggingClassifier(estimator= dt_model, bootstrap= True,n_estimators=60)\n",
    "bagg_rf_model = RandomForestClassifier(bootstrap= True, max_depth= 10,max_features = 'sqrt', min_samples_leaf=4, min_samples_split= 10, n_estimators= 100, random_state=123)\n",
    "\n",
    "bagg_gb_model = GradientBoostingClassifier( learning_rate = 0.10690382853418266, max_depth = 6,\n",
    "                                      max_features = None, min_samples_leaf = 8, min_samples_split = 14,\n",
    "                                      n_estimators = 296, subsample = 0.9503681331729179, random_state=123)\n",
    "\n",
    "xgb_model = XGBClassifier( colsample_bylevel= 0.7005087783306018, colsample_bytree = 0.8136585046688083, gamma = 0.16207544668977053,\n",
    "                           learning_rate = 0.17342778308617596, max_depth = 7, min_child_weight = 1, n_estimators = 118, \n",
    "                           objective = 'multi:softmax', reg_alpha = 0.6317920176870504, reg_lambda = 0.44025717806407627, \n",
    "                           scale_pos_weight = 1.8372648456358434, subsample = 0.8561650906943812, random_state=123 )\n",
    "\n",
    "lr_pipe = Pipeline(steps=[('anova',SelectKBest(score_func=f_classif, k=8)),  ('scaler', MinMaxScaler()),  ('lr', LogisticRegression(C =100, max_iter=1000,solver='newton-cg'))])\n",
    "\n",
    "svc_pipe = Pipeline( steps=[('scaler',MinMaxScaler()),  ('svc', SVC(C = 1000, coef0 = 1.0, degree = 1, gamma = 'scale', kernel = 'sigmoid'))])\n",
    "# KNN scaling\n",
    "\n",
    "\n",
    "estimators = [ (\"bagg_dt\", bagg_dt_model), (\"bagg_rf\",bagg_rf_model), \n",
    "               (\"bagg_gb\" , bagg_gb_model), ( \"xgb_model\",xgb_model),\n",
    "         #     (\"SVC\", SVC(random_state=123)), \n",
    "              (\"lr_pipe\", lr_pipe),\n",
    "              (\"KNN\", KNeighborsClassifier(metric='euclidean', n_neighbors=7,weights='uniform')),\n",
    "            #  (\"gnb\",  Pipeline( steps=[('scaler',MinMaxScaler()),('gnb', GaussianNB()) ]))\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 1 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:39:14] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:49:05] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:49:12] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:49:19] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:49:25] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [03:49:32] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier(estimators=[('bagg_dt',\n",
      "                                BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=4,\n",
      "                                                                                   random_state=2022),\n",
      "                                                  n_estimators=60)),\n",
      "                               ('bagg_rf',\n",
      "                                RandomForestClassifier(max_depth=10,\n",
      "                                                       min_samples_leaf=4,\n",
      "                                                       min_samples_split=10,\n",
      "                                                       random_state=123)),\n",
      "                               ('bagg_gb',\n",
      "                                GradientBoostingClassifier(learning_rate=0.10690382853418266,\n",
      "                                                           max_depth=6,\n",
      "                                                           min_samples_leaf=8...\n",
      "                                              n_estimators=118, n_jobs=None,\n",
      "                                              num_parallel_tree=None,\n",
      "                                              objective='multi:softmax', ...)),\n",
      "                               ('lr_pipe',\n",
      "                                Pipeline(steps=[('anova', SelectKBest(k=8)),\n",
      "                                                ('scaler', MinMaxScaler()),\n",
      "                                                ('lr',\n",
      "                                                 LogisticRegression(C=100,\n",
      "                                                                    max_iter=1000,\n",
      "                                                                    solver='newton-cg'))])),\n",
      "                               ('KNN',\n",
      "                                KNeighborsClassifier(metric='euclidean',\n",
      "                                                     n_neighbors=7))],\n",
      "                   final_estimator=RandomForestClassifier(random_state=123))\n",
      "Best Mean Macro F1: 0.327\n",
      "Best Config: {'final_estimator': RandomForestClassifier(random_state=123)}\n"
     ]
    }
   ],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "\n",
    "\n",
    "st_model = StackingClassifier(estimators=estimators,final_estimator=RandomForestClassifier())\n",
    "\n",
    "# st_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "grid_st = {\n",
    "    'final_estimator' : [\n",
    "                          #LogisticRegression(random_state=123),\n",
    "                          RandomForestClassifier(random_state=123)\n",
    "                          ]\n",
    "}\n",
    "\n",
    "\n",
    "grid_rf = GridSearchCV(estimator= st_model, \n",
    "                       param_grid= grid_st,\n",
    "                       cv = cv,\n",
    "                       refit=True,return_train_score=True, \n",
    "                       scoring= \"f1_macro\", n_jobs=-1, verbose= 1)\n",
    "\n",
    "grid_rf.fit(X_train,y_train)\n",
    "\n",
    "print(grid_rf.best_estimator_)\n",
    "\n",
    "print('Best Mean Macro F1: %.3f' %  grid_rf.best_score_)\n",
    "print('Best Config: %s' % grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackingClassifier(estimators=[('bagg_dt',\n",
    "#                                 BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=4,\n",
    "#                                                                                    random_state=2022),\n",
    "#                                                   n_estimators=60)),\n",
    "#                                ('bagg_rf',\n",
    "#                                 RandomForestClassifier(max_depth=10,\n",
    "#                                                        min_samples_leaf=4,\n",
    "#                                                        min_samples_split=10,\n",
    "#                                                        random_state=123)),\n",
    "#                                ('bagg_gb',\n",
    "#                                 GradientBoostingClassifier(learning_rate=0.10690382853418266,\n",
    "#                                                            max_depth=6,\n",
    "#                                                            min_samples_leaf=8...\n",
    "#                                               n_estimators=118, n_jobs=None,\n",
    "#                                               num_parallel_tree=None,\n",
    "#                                               objective='multi:softmax', ...)),\n",
    "#                                ('lr_pipe',\n",
    "#                                 Pipeline(steps=[('anova', SelectKBest(k=8)),\n",
    "#                                                 ('scaler', MinMaxScaler()),\n",
    "#                                                 ('lr',\n",
    "#                                                  LogisticRegression(C=100,\n",
    "#                                                                     max_iter=1000,\n",
    "#                                                                     solver='newton-cg'))])),\n",
    "#                                ('KNN',\n",
    "#                                 KNeighborsClassifier(metric='euclidean',\n",
    "#                                                      n_neighbors=7))],\n",
    "#                    final_estimator=RandomForestClassifier(random_state=123))\n",
    "# Best Mean Macro F1: 0.327\n",
    "# Best Config: {'final_estimator': RandomForestClassifier(random_state=123)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "#submitModel(grid_rf.best_estimator_.predict(X_test),'test_predictions_stacking_f1_0327')\n",
    "#saveModel(grid_rf.best_estimator_,\"stacking4_f1_0327\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
