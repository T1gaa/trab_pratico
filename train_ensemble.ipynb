{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import preprocessingV1\n",
    "from utils.modelization import saveModel,loadModel,submitModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from scipy.stats import uniform, randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"datasets/train_radiomics_hipocamp.csv\")\n",
    "df_test = pd.read_csv(\"datasets/test_radiomics_hipocamp.csv\")\n",
    "\n",
    "X_full_train, y_full_train = preprocessingV1(df_train)\n",
    "\n",
    "X_test = preprocessingV1(df_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_full_train,y_full_train,test_size=0.1,stratify=y_full_train, random_state=1234)\n",
    "\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging with Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train macro F1-Score 0.3525085731262202\n",
      "Val Macro F1-Score: 0.35238095238095235\n",
      "BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=4,\n",
      "                                                   random_state=2022),\n",
      "                  n_estimators=60)\n",
      "{'estimator__max_depth': 4, 'n_estimators': 60}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       AD-AD       0.75      0.50      0.60         6\n",
      "       CN-CN       0.73      0.80      0.76        10\n",
      "      CN-MCI       1.00      0.00      0.00         1\n",
      "      MCI-AD       0.25      0.29      0.27         7\n",
      "     MCI-MCI       0.12      0.14      0.13         7\n",
      "\n",
      "    accuracy                           0.45        31\n",
      "   macro avg       0.57      0.35      0.35        31\n",
      "weighted avg       0.50      0.45      0.45        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size= 20, random_state= 2022)\n",
    "dt_model = DecisionTreeClassifier(max_depth=2,random_state=2022)\n",
    "bg_model = BaggingClassifier(estimator= dt_model, bootstrap= True)\n",
    "\n",
    "grid_p = { \n",
    "    'n_estimators' : [20,40,60,80,100],\n",
    "    'estimator__max_depth' : [2,4,6] \n",
    "    }\n",
    "\n",
    "grid_bg = GridSearchCV(estimator= bg_model, param_grid= grid_p, cv = sss,refit=True,return_train_score=True, scoring= \"f1_macro\", n_jobs=-1)\n",
    "grid_bg.fit(X_train,y_train)\n",
    "\n",
    "bst_bg_model = grid_bg.best_estimator_\n",
    "bst_bg_score = bst_bg_model.score(X_val,y_val)\n",
    "\n",
    "y_preds = bst_bg_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_preds, average='macro')\n",
    "\n",
    "results['Bagg'] = macro_f1\n",
    "models['Bagg'] = bst_bg_model\n",
    "\n",
    "\n",
    "print(\"Train macro F1-Score\", grid_bg.best_score_)\n",
    "print(\"Val Macro F1-Score:\", macro_f1)\n",
    "\n",
    "print(bst_bg_model)\n",
    "print(grid_bg.best_params_)\n",
    "\n",
    "bst_bg_predictions = bst_bg_model.predict(X_val)\n",
    "print(classification_report(y_val,bst_bg_predictions, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31883053221288515"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Train macro F1-Score 0.3525085731262202\n",
    "# Val Macro F1-Score: 0.35238095238095235\n",
    "# BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=4,\n",
    "#                                                   random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3525085731262202\n"
     ]
    }
   ],
   "source": [
    "print(grid_bg.best_score_)\n",
    "\n",
    "#saveModel(grid_bg.best_estimator_,\"cv_bagging_dt_mf1_3525\")\n",
    "#submitModel(grid_bg.best_estimator_.predict(X_test),'test_predictions_bagging_dt_mf1_03525')\n",
    "\n",
    "#Train macro F1-Score 0.3525085731262202\n",
    "#Val Macro F1-Score: 0.35238095238095235\n",
    "#BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=4,\n",
    " #                                                  random_state=2022),\n",
    " #                 n_estimators=60)\n",
    "#{'estimator__max_depth': 4, 'n_estimators': 60}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 216 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Macro F1: 0.326\n",
      "Best Config: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.32618107236807714\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(bootstrap= True, max_depth= 2, verbose= 1, random_state=123)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "\n",
    "grid_p = { \n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10,20],\n",
    "    'min_samples_leaf': [1, 2, 4,8],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "grid_rf = GridSearchCV(estimator= rf_model, \n",
    "                       param_grid= grid_p,\n",
    "                       cv = cv,\n",
    "                       refit=True,return_train_score=True, \n",
    "                       scoring= \"f1_macro\", n_jobs=-1, verbose= 1)\n",
    "grid_rf.fit(X_full_train,y_full_train)\n",
    "\n",
    "results['Bagg_rf'] =  grid_rf.best_score_\n",
    "models['Bagg_rf'] =  grid_rf.best_estimator_\n",
    "\n",
    "print('Best Mean Macro F1: %.3f' %  grid_rf.best_score_)\n",
    "print('Best Config: %s' % grid_rf.best_params_)\n",
    "\n",
    "print(grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saveModel(grid_rf.best_estimator_,\"cv_bagging_rf_mf1_0326\")\n",
    "\n",
    "\n",
    "\n",
    "# Best Mean Macro F1: 0.326\n",
    "# Best Config: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.2062           0.0976            3.65m\n",
      "         2           1.0418           0.1662            3.96m\n",
      "         3           0.9094           0.0511            4.07m\n",
      "         4           0.7982           0.2378            4.13m\n",
      "         5           0.7010          -0.0151            4.18m\n",
      "         6           0.6171           0.1233            4.17m\n",
      "         7           0.5467           0.1387            4.18m\n",
      "         8           0.4882           0.1458            4.20m\n",
      "         9           0.4280           0.0690            4.18m\n",
      "        10           0.3746          -0.0014            4.18m\n",
      "        20           0.1247           0.0224            4.13m\n",
      "        30           0.0450          -0.0009            4.03m\n",
      "        40           0.0157          -0.0005            3.90m\n",
      "        50           0.0060           0.0013            3.75m\n",
      "        60           0.0023           0.0008            3.59m\n",
      "        70           0.0008           0.0001            3.44m\n",
      "        80           0.0003           0.0000            3.29m\n",
      "        90           0.0001          -0.0000            3.14m\n",
      "       100           0.0000           0.0000            2.99m\n",
      "       200           0.0000           0.0000            1.26m\n",
      "Best Mean Macro F1: 0.339\n",
      "Best Config: {'learning_rate': 0.10690382853418266, 'max_depth': 6, 'max_features': None, 'min_samples_leaf': 8, 'min_samples_split': 14, 'n_estimators': 296, 'subsample': 0.9503681331729179}\n",
      "0.3385127160334949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gbc_model = GradientBoostingClassifier( verbose= 1, random_state=123)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
    "\n",
    "\n",
    "param_distrib = {\n",
    "    'n_estimators': randint(50, 500),           # Number of boosting stages\n",
    "    'learning_rate': uniform(0.01, 0.3),       # Step size shrinkage\n",
    "    'max_depth': randint(3, 100),               # Maximum depth of each tree\n",
    "    'min_samples_split': randint(2, 20),       # Minimum samples to split an internal node\n",
    "    'min_samples_leaf': randint(1, 10),        # Minimum samples required at a leaf node\n",
    "    'subsample': uniform(0.6, 0.4),            # Fraction of samples used per tree\n",
    "    'max_features': ['sqrt', 'log2', None]     # Features considered for the best split\n",
    "}\n",
    "\n",
    "grid_gbc = RandomizedSearchCV(estimator= gbc_model, \n",
    "                       param_distributions= param_distrib,\n",
    "                       n_iter=10,\n",
    "                       cv = cv,\n",
    "                       refit=True,return_train_score=True, \n",
    "                       scoring= \"f1_macro\", n_jobs=-1, verbose= 1, random_state=123)\n",
    "\n",
    "grid_gbc.fit(X_full_train,y_full_train)\n",
    "\n",
    "results['Bagg_gbc'] =  grid_gbc.best_score_\n",
    "models['Bagg_gbc'] =  grid_gbc.best_estimator_\n",
    "\n",
    "print('Best Mean Macro F1: %.3f' %  grid_gbc.best_score_)\n",
    "print('Best Config: %s' % grid_gbc.best_params_)\n",
    "\n",
    "print(grid_gbc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(grid_gbc.best_estimator_,\"cv_gbc_mf1_0339\")\n",
    "# Best Config: {'learning_rate': 0.10690382853418266, 'max_depth': 6, 'max_features': None, 'min_samples_leaf': 8, 'min_samples_split': 14, 'n_estimators': 296, 'subsample': 0.9503681331729179}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 2 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\.conda\\envs\\mypython3v\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:50:41] WARNING: D:\\bld\\xgboost-split_1732667012888\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Macro F1: 0.346\n",
      "Best Config: {'colsample_bylevel': 0.7005087783306018, 'colsample_bytree': 0.8136585046688083, 'gamma': 0.16207544668977053, 'learning_rate': 0.17342778308617596, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 118, 'objective': 'multi:softmax', 'reg_alpha': 0.6317920176870504, 'reg_lambda': 0.44025717806407627, 'scale_pos_weight': 1.8372648456358434, 'subsample': 0.8561650906943812}\n",
      "0.3455182303834074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "xgb_model = XGBClassifier( verbose= 1, random_state=123)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=123)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),  # Number of boosting rounds (trees)\n",
    "    'learning_rate': uniform(0.1, 0.3),  # Step size shrinking, in the range [0.01, 0.3]\n",
    "    'max_depth': randint(3, 15),  # Maximum depth of the decision tree\n",
    "    'min_child_weight': randint(1, 15),  # Minimum sum of instance weight (hessian) for a child\n",
    "    'subsample': uniform(0.5, 0.5),  # Fraction of samples to use for fitting (0.5 to 1)\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # Fraction of features to use for each tree\n",
    "    'colsample_bylevel': uniform(0.5, 0.5),  # Fraction of features to use for each level\n",
    "    'gamma': uniform(0, 0.5),  # Minimum loss reduction to make a further partition\n",
    "    'reg_alpha': uniform(0, 1),  # L1 regularization term on weights\n",
    "    'reg_lambda': uniform(0, 1),  # L2 regularization term on weights\n",
    "    'objective': ['multi:softmax'],  # Objective function (binary or multi-class)\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV \n",
    "rs_xgb = RandomizedSearchCV(estimator= xgb_model, \n",
    "                       param_distributions= param_distributions,\n",
    "                       n_iter=2,\n",
    "                       cv = cv,\n",
    "                       refit=True,return_train_score=True, \n",
    "                       scoring= \"f1_macro\", n_jobs=-1, verbose= 1, random_state=123)\n",
    "\n",
    "rs_xgb.fit(X_full_train, label_encoder.fit_transform(y_full_train) )\n",
    "\n",
    "results['rs_xgb'] =  rs_xgb.best_score_\n",
    "models['rs_xgb'] =  rs_xgb.best_estimator_\n",
    "\n",
    "print('Best Mean Macro F1: %.3f' %  rs_xgb.best_score_)\n",
    "print('Best Config: %s' % rs_xgb.best_params_)\n",
    "\n",
    "print(rs_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 2 1 3 1 0 4 4 1 0 0 0 3 3 1 4 3 3 0 1 3 4 1 1 0 1 1 1 1 3 4 3 1 0 2\n",
      " 1 0 3 1 3 0 1 0 3 0 0 4 4 3 0 1 1 4 0 4 1 4 1 1 4 4 1 2 1 4 0 4 0 4 2 0 4\n",
      " 1 3 1 1 1 3 0 0 1 4 1 3 0 4 1 4 0 4 0 1 2 4 4 4 3 3 3 0 4 3 3 4 0 4 4 4 0\n",
      " 4 1 3 1 4 4 0 1 4 3 0 3 1 4 4 1 1 1 0 3 1 0 3 4 3 1 1 4 3 4 4 0 0 1 0 2 0\n",
      " 4 0 1 1 3 3 4 3 1 4 0 3 1 3 1 4 0 0 1 4 4 3 4 3 2 1 0 3 1 3 1 4 1 4 4 4 3\n",
      " 1 1 1 3 1 1 0 0 1 3 0 3 3 0 1 1 1 1 0 3 1 3 3 4 3 1 0 1 1 4 0 1 1 0 4 0 0\n",
      " 3 2 1 4 3 1 1 1 3 4 1 4 3 2 3 3 3 3 0 1 3 0 1 1 3 3 4 1 1 0 1 0 4 4 0 3 3\n",
      " 3 4 4 1 0 4 0 1 3 1 4 1 1 4 4 1 1 3 1 3 3 1 4 0 3 4 4 1 3 0 3 0 4 0 1 1 3\n",
      " 4 1 4 2 1 1 3 4 1]\n",
      "0        CN-CN\n",
      "1        CN-CN\n",
      "2        AD-AD\n",
      "3       CN-MCI\n",
      "4        CN-CN\n",
      "        ...   \n",
      "300      CN-CN\n",
      "301      CN-CN\n",
      "302     MCI-AD\n",
      "303    MCI-MCI\n",
      "304      CN-CN\n",
      "Name: Transition, Length: 305, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Best Mean Macro F1: 0.346\n",
    "# Best Config: {'colsample_bylevel': 0.7005087783306018, 'colsample_bytree': 0.8136585046688083, 'gamma': 0.16207544668977053, 'learning_rate': 0.17342778308617596, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 118, 'objective': 'multi:softmax', 'reg_alpha': 0.6317920176870504, 'reg_lambda': 0.44025717806407627, 'scale_pos_weight': 1.8372648456358434, 'subsample': 0.8561650906943812}\n",
    "\n",
    "\n",
    "print(label_encoder.fit_transform(y_full_train))\n",
    "print(y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#saveModel(rs_xgb.best_estimator_,\"cv_xgboost_mf1_0346\")\n",
    "\n",
    "#saveModel(grid_bg.best_estimator_,\"cv_bagging_dt_mf1_3525\")\n",
    "submitModel(  label_encoder.inverse_transform(rs_xgb.best_estimator_.predict(X_test)),'test_predictions_xgboost_mf1_0346')\n",
    "\n",
    "\n",
    "#Best Mean Macro F1: 0.346\n",
    "#Best Config: {'colsample_bylevel': 0.7005087783306018, 'colsample_bytree': 0.8136585046688083, 'gamma': 0.16207544668977053, 'learning_rate': 0.17342778308617596, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 118, 'objective': 'multi:softmax', 'reg_alpha': 0.6317920176870504, 'reg_lambda': 0.44025717806407627, 'scale_pos_weight': 1.8372648456358434, 'subsample': 0.8561650906943812}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = search.best_estimator_\n",
    "y_test_results = best_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Mean Macro F1: 0.328\n",
    "#Best Config: {'scaler': MinMaxScaler(), 'svc__C': 100, 'svc__degree': 1, 'svc__gamma': 'auto', 'svc__kernel': 'poly'}\n",
    "#saveModel(best_pipeline,\"svc_predictions_svc_mf1_0328_poly_submission_022811\")\n",
    "\n",
    "\n",
    "\n",
    "#Best Mean Macro F1: 0.347\n",
    "#Best Config: {'scaler': MinMaxScaler(), 'svc__C': 1000, 'svc__coef0': 1.0, 'svc__degree': 1, 'svc__gamma': 'scale', 'svc__kernel': 'sigmoid'}\n",
    "#saveModel(best_pipeline,\"svc_mf1_0347_sigmoid_submission_028741\")\n",
    "\n",
    "# loadedm = loadModel(\"best_models/lr_anova_local_0354_submission_039047.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
